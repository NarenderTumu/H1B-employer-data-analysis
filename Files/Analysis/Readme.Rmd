---
title: "H1b Employer Data Analysis"
author: "Narender Tumu"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r Loading packages,message=FALSE,warning=FALSE,include=FALSE}
# Loading the necessary packages
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)
library(tidyverse)
library(tidymodels)
library(readxl)
library(readr)
library(ggplot2)
library(pscl)
library(glmnet)
library(treemap)
library(GGally)
library(ggfortify)
library(caret)
library(MASS)

```



## Loading the "H1b_data.RData" file which was obtained after cleaning the raw data files. For the process of cleaning the data navigate to [Data Cleaning](https://github.com/NarenderTumu/H1B-employer-data-analysis/tree/main/Files/Data%20Cleaning)



```{r Loading data,warning=FALSE,include=FALSE}
load("~/STA631/H1B-employer-data-analysis/Files/Data Cleaning/H1b_data.RData")
```

#### Below are the Data dictionaries to understand the data better

**Data Dictionary for the H1B employer data**

```{r creating a Data Dictionary,echo=FALSE}
# Define the data dictionary as a data frame
dict <- data.frame(
  Variable = names(H1b_data),
  Description = c("Petitioner’s five-digit ZIP code ",
                  "This variable indicates the fiscal year in which the H-1B petition was filed. Fiscal year in the United States starts from October 1st and ends on September 30th of the following year.", 
                  "Petitioner’s firm/employer name", 
                  "Number of approvals when the employer filing petition for the first time to the worker",
                  "Number of denails when the employer filing petition for the first time to the worker",
                  "Number of approvals when the employer is filing the petition for the worker who was already having an approved visa for the same employer and wishing to extend the workers time period",
                  "Number of denials when the employer is filing the petition for the worker who was already having an approved visa for the same employer and wishing to extend the workers time period",
                  "North American Industry Classification System Code: A character string that stands for an industry classification within the North American Industry Classification System,For more information on the NAICS, visit the [U.S. Census Bureau’s North American Industry Classification Code webpage](https://www.census.gov/naics/)",
                  "Petitioner’s state",
                  "Petitioner’s City",
                  "Population of the place by Zipcode"), 
Datatype = sapply(H1b_data, class),stringsAsFactors = FALSE)


kable(dict,align = "lccc", row.names = FALSE ) %>%
kable_styling("bordered",full_width = FALSE,position = "center")
```




**The following table provides detailed information on the definitions of NAICS codes**



```{r NAICS data dictionary,echo=FALSE}
NAICS_dictionary<-data.frame(Sector=c("11","21","22","23","31-33","42","44-45","48-49","51","52","53","54","55","56","61","62","71","72","81","92","99"),Definition=c("Agriculture, Forestry, Fishing and Hunting","Mining, Quarrying, and Oil and Gas Extraction","Utilities","Construction","Manufacturing","Wholesale Trade","Retail Trade","Transportation and Warehousing","Information","Finance and Insurance","Real Estate and Rental and Leasing","Professional, Scientific, and Technical Services","Management of Companies and Enterprises","Administrative and Support and Waste Management and Remediation Services","Educational Services","Health Care and Social Assistance","Arts, Entertainment, and Recreation","Accommodation and Food Services","Other Services (except Public Administration)","Public Administration","Unknown"))

kable(NAICS_dictionary, row.names = FALSE ) %>%
  kable_styling("bordered",full_width = FALSE,position = "center")

```




we have variables that are numeric, character and integer. Let's convert some of the variables to use them for analysis


```{r Changing Data types}
# converting the Fiscal_Year, NAICS variables from numeric to factor

H1b_data$Fiscal_Year<-as.factor(H1b_data$Fiscal_Year)

H1b_data$NAICS<-as.factor(H1b_data$NAICS)


# converting the State, City variables from Character to factor

H1b_data$State<-as.factor(H1b_data$State)

H1b_data$City<-as.factor(H1b_data$City)
```



Checking the Summary statistics of all the numeric variables in the data set using select() and summary() functions



```{r summary of numeric variables,echo=FALSE}

Numeric<-H1b_data[,c(4,5,6,7,11)]
summary(Numeric)

```


* Using the table() function for all the factor variables to get the frequency distribution of each level


* We can see that from the below plot, most of the employers belongs to Professional, Scientific, and Technical Services category


```{r NAICS column chart,echo=FALSE}
NAICS_frequency<-data.frame(table(H1b_data$NAICS))


ggplot(NAICS_frequency,aes(x=reorder(Var1,Freq),y=Freq))+
  geom_col(color="black",fill="blue")+
  labs(title="Number of employers based on their NAICS classification",
       x="NAICS code",
       y="Number of employers",
       caption = "Data source: USCIS H1B Employer Datahub 2017-2021")+
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(color = "black"),axis.text.y = element_text(color = "black"))

```



* We can see from the below chart, that California has the highest number of employees filing for H1B visa followed by New York, Texas, New Jersey


```{r States Column Chart,echo=FALSE}
State_frequency<-data.frame(table(H1b_data$State))


ggplot(State_frequency,aes(x=reorder(Var1,Freq),y=Freq,fill=Freq))+
  geom_col(color="black")+
  scale_fill_gradient(low = "blue", high = "orange")+
  labs(title="Number of employers filed petitions from each state (2017-2021)",
       x="State",
       y="Number of employers",
       caption = "Data source: USCIS H1B Employer Datahub 2017-2021")+
    theme_light()+
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 90, hjust = 1,colour = "black"),axis.text.y = element_text(colour = "black"))
```

* We can see from the below tree map of top 30 cities in US based on the number of employers filing H1B petitions from 2017-2021, where New York, San Francisco, Houston, Chicago, Boston stands in the top

```{r City Tree map,echo=FALSE,warning=FALSE}

City_frequency<-data.frame(table(H1b_data$City))
City_frequency<-City_frequency%>% arrange(desc(Freq))

Top30_cities<-City_frequency[1:30,]

Top30_cities$Freq<-as.numeric(Top30_cities$Freq)

treemap(Top30_cities,
        index = "Var1",
        vSize = "Freq",
        vColor="Freq",
        type = "value",
        palette = "YlOrBr",
        title = "Top 30 US Cities by employers filing H1B Petitions, 2017-2021")
```


#### Checking the distribution of variables and correlations between the numerical variables

 * Here are the histograms for all the numeric variables. We can observe that the variables Initial_Approvals, Initial_Denials, Continuing_Approvals, and Continuing_Denials are heavily right skewed, while the Total_population variable is only slightly right skewed.


 
```{r histograms of numeric variables,echo=FALSE}
hist(H1b_data$Initial_Approvals)
hist(H1b_data$Initial_Denials)
hist(H1b_data$Continuing_Approvals)
hist(H1b_data$Continuing_Denials)
hist(H1b_data$Total_population)
```


* Below we can see the output of ggpairs() which gives histograms and scatter plots of all the numeric variables with their respective correlation-coefficients from which we can see there is some correlation between Initial_Approvals and Continuing Approvals. 


```{r GGpairs of all numeric variables,warning=FALSE,echo=FALSE}
ggpairs(Numeric)
```


* This Scatter plot below explains the relationship between Initial and Continuing Approvals

```{r scatter plot b/w initial and continuing approvals,echo=FALSE}
ggplot(H1b_data,aes(x=Continuing_Approvals,y=Initial_Approvals))+
  geom_point(color="red")+
  labs(title = "Scatterplot between Initial and Continuing Approvals",
       x="Continuing Approvals",
       y="Initial Approvals",
       caption = "Data source: USCIS H1B Employer Datahub 2017-2021")+
  theme(plot.title = element_text(hjust = 0.5),plot.caption.position = "panel")
```


### To initiate the analysis, I plan to test the models on a smaller dataset due to the large size of our main dataset. I will filter the data based on fiscal year "2021" to focus on the H1B employer data for that year. Then, I will divide the 2021 data into two subsets: a training set(70% of the data) and a testing set(30% of the data), for further analysis.


```{r filtering and splitting data,echo=TRUE}
H1b_data2021<-H1b_data%>%filter(Fiscal_Year==2021)

set.seed(1234)

trainIndex <- createDataPartition(H1b_data2021$Initial_Approvals, p = 0.7, list = FALSE, times = 1)
train <- H1b_data2021[trainIndex,]
test <- H1b_data2021[-trainIndex,]
```


* I have created two new variables by adding a constant and taking log of that since the response and predictor variable are heavily right skewed and have a lot of zeros in the data.

```{r transforming variables}
train$logIN_Approvals<-log(train$Initial_Approvals+2)
train$logCON_Approvals<-log(train$Continuing_Approvals+2)
```


### Model 1- Simple linear Regression

* The following linear model is created to forecast initial approvals based on continuing approvals. Both variables are transformed because they are skewed and non-normal. The deviance residuals in the summary look decent, except for the unusually high minimum and maximum deviance in residuals. The coefficients and p-values for the intercept and predictor variable are acceptable and significant in predicting the response variable. The R-squared and adjusted R-squared values are low, indicating that the model only accounts for 32% of the variance. The model failed to meet the assumptions reasonably well, it does not perform well in predicting the response variable.

```{r simple linear model, echo=FALSE}
Model1<-lm(logIN_Approvals~logCON_Approvals,data = train)
summary(Model1)
autoplot(Model1, label.size = 2) + theme_bw()
```

### Model 2- Multiple linear Regression


* I have used NAICS and transformed Continuing_Approvals to predict transformed Initial_Approvals in this multiple regression model which we can see below. This model is almost similar to the simple regression model in terms of performance, so we can say the earlier simple linear regression is better performed than this model to predict the response variable. Let's try some other model


```{r multiple linear regression model,echo=FALSE}
Model2<-lm(logIN_Approvals~logCON_Approvals+NAICS,data = train)
summary(Model2)
autoplot(Model2)
```


### Model3- Poisson regression


* I have built a poisson regression model using Continuing approvals to predict the initial approvals since both of them are count variables and non-normal. The summary of the model seems not so promising in predicting the Initial_Approvals. Let's move to the next model


```{r poisson regression model,echo=FALSE}
Model3<-glm(Initial_Approvals ~ Continuing_Approvals, data = train, family = poisson)
summary(Model3)
```





## Up to this point, I have attempted to construct various statistical models, such as simple and multiple linear regression models, poisson regression models. These models were intended to predict the response variable "Initial_Approvals" using  Continuing_Approvals and NAICS. Simple linear Regression performs better of these three models. Unfortunately, my attempts to predict the response variable have not been particularly successful. It's possible that I need to change my approach or look for errors in my model building. There is still a great deal of future work that needs to be done on this project. If you have any suggestions or would like to discuss the project further, [please to reach out to me](https://github.com/NarenderTumu/H1B-employer-data-analysis/issues/1).


